\documentclass[10pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{braket}
\usepackage{dsfont}
\usepackage{tcolorbox}
\usepackage[a4paper, margin=1in]{geometry}

\newcommand{\red}{\color{red}}
\newcommand{\blue}{\color{blue}}
\newcommand{\magenta}{\color{magenta}}
\newcommand{\black}{\color{black}}
\newcommand{\gray}{\color{gray}}

\definecolor{orange}{HTML}{FC4820}
\newcommand{\orange}{\color{orange}}

\definecolor{lblue}{HTML}{2086B0}
\newcommand{\lblue}{\color{lblue}}

\definecolor{lgreen}{HTML}{57A04D}
\newcommand{\lgreen}{\color{lgreen}}

\definecolor{dgray}{HTML}{4D4D4D}
\newcommand{\dgray}{\color{dgray}}



\def\half{{1 \over 2}}
\def\d{{\rm d}}  % Roman d for derivatives
\def\O{{\cal O}}
\def\expect#1{\left\langle #1 \right\rangle} % expectation value < >
\def\eq{Eq.~}
\def\eqs{Eqs.~}
\def\eqpar#1{(#1)} % Used to put a number in parentheses and to
                   % indicate to the renumbering program that it
                   % is an equation number.
\def\ds{\displaystyle}
\def\underarrow#1{\mathrel{\mathop{\Longrightarrow}\limits_{#1}}}
\def\implies{\quad \Longrightarrow \quad}
\def\Tr{\mathop{\rm Tr}}
\def\intinf{\int_{-\infty}^{\infty}}
\def\diag{\mathop{\rm diag}\nolimits}
\def\min{{\rm min}}


\title{Numerical Calculation of the Peak Density Integral in two Squared-Gaussian Fields}

\begin{document}
\maketitle

\section{Background}

In a series of three write-ups FIXME CITE, Professor Guth (and earlier, Saarik Kalia) has written down integral formulations of the expected spatial number density of both minima and stationary points in a sum-of-squares gaussian field ($\Phi = \phi_1 + \phi_2$). The general tactic is similar to the approach in BBKS (FIXME cite bbks). The integrand contains a delta function in the gradient of $\Phi$ and a delta function enforcing the constraint that $\phi_1$ attain a value some number of standard deviations away from the field mean (zero): $\phi_1 = \nu \sigma_0$, where $\sigma_0$ is the standard deviation in $\phi_1$. Additionally, the function contains a factor of the determinant of the hessian matrix of $\Phi$ to compensate for the Jacobian factor from the delta-gradient integration. Cancelling out the Jacobian forces the delta function to contribute a value of $\pm 1$ whenever the gradient-delta fires at a stationary point. The sign of the contribution will be addressed shortly. Two central techniques are employed to beat the problem into a useful shape. The first is a rotation in field-space, which will remain an important method as the number of fields is increased. If one envisions $\phi_1$ and $\phi_2$ as the coeffiecients of a pair of orthonormal basis vectors in a plane, then the expression $\Phi = \phi_1^2 + \phi_2^2$ is invariant under rotations in this plane. Consequently, the fields can be rotated so that only $\bar{\phi}_1$ is non-zero, and the rotational invariance of our problem implies that the rotated coordinates are just as good as the original ones, so we are able to set $\bar{\phi}_2$ to zero, greatly simplifying our expression (though the gradient of $\phi_2$ is not generally zero, and will play a role in the calculation of the hessian matrix of $\Phi$. 

\par The second technique is a transformation into the eigenbasis of the hessian matrix. This allows us to calculate in terms of curvature eigenvalues and rotation matrices (which constitute variable transformations of unit jacobian, and are therefore integral-friendly). Without descending into too much detail (which is nicely worked out in the write-ups), direct control of the eigenvalues allows us to restrict ourselves to minima (all positive eigenvalues), maxima (all negative), or arbitrary saddle-point mixtures. The physically interesting quantity is the number density of field minima, so we will integrate over the positive octant of [$\lambda_1, \lambda_2, \lambda_3$] space, where the lambdas are these curvature eigenvalues. Our density is independent of the rotation matrices which map this eigen-space onto our physical 3-space, so their euler angles integrate out trivially to a prefactor.
\par The final step is to calculate the expectation of this density integral over all possible field configurations. We have boiled the density integral down to the value of $\phi_1$ ($\nu \sigma_0$), the hessian eigenvalues $\lambda_1$ - $\lambda_3$, and the gradient components of $\phi_2$: $\eta_1$ - $\eta_3$. Instead of integrating over 3-space to calculate the total number of minima, we will leave the spatial integrals off and instead integrate over the configuration parameters $\lambda_i$ and $\eta_j$, yielding an expectation of number density. Our field is gaussian, so the joint pdf of these 6 parameters is a multivariate gaussian with covariance matrix calculated in detail in FIXME cite. Putting all of this together yields the following formula from FIXME cite.

\begin{gather} \label{eq:allspace_integral}
\expect{X}_{Q_2} = \frac{\displaystyle 30,375 \sigma_0^6}{\displaystyle 16 \sqrt{30} \pi^{5/2} \kappa^5 \sigma_1^{15}e^{-\nu^2/2} \sqrt{\kappa^2-1}} \times \int_{-\infty}^{\infty} \d \lambda_1 
     \int_{-\infty}^{\lambda_1} \d \lambda_2 
     \int_{-\infty}^{\lambda_2} \d \lambda_3 (\lambda_1 -
     \lambda_2) (\lambda_1 - \lambda_3) (\lambda_2 - \lambda_3) \\
     \times \intinf \d \eta_{3} 
     \intinf \d \eta_{2} 
     \intinf \d \eta_{1}  \times  e^{- Q_2} \nonumber
\end{gather}


Where:

\begin{gather} \label{eq:Q2}
Q_2 = \half \nu^2 + {3 \over 2 \sigma_1^2} \sum_{i} \nonumber
     \eta_{i}^2
     {5 \sigma_0^2 \over 4 \kappa^2 \sigma_1^4} \left[3
     \left( \sum_i \lambda_i^2 - {2 \over \nu \sigma_0} \sum_i
     \lambda_i  n_i^2 + {1 \over \nu^2 \sigma_0^2} \left(
     \sum_i \eta_i^2 \right)^2\right) - \left(\sum_i
     \lambda_i - {1 \over \nu \sigma_0} \sum_i \eta_i^2
     \right)^2\right]\\ 
     {1 \over 2 \sigma_0^2 (\kappa^2 - 1)}
     \left[\nu \sigma_0 + {\sigma_0^2 \over \sigma_1^2}
     \left(\sum_i \lambda_i - {1 \over \nu \sigma_0} \sum_i
     \eta_i^2 \right)\right]^2 \\ \nonumber
\end{gather}


In \ref{eq:allspace_integral}, the integral is taken over $\mathbb{R}^3$ in $\eta$, but is taken over an ordered subspace of $\mathbb{R}^3$ in $\lambda$. This is an enforcement of the restriction:

\begin{equation} \label{eq:ordering}
\lambda_1 \geq \lambda_2 \geq \lambda_3
\end{equation}

Only after choosing such an ordering of eigenvalues can one guarantee unique``diagonal'' decomposition of the hessian into a product of $SO(3)$ matrices and eigenvalues. Without this ordering in place, there is a multiplicity factor of $\frac{1}{3!}$ that appears in front of the integral.

\section{Attempts at an Analytic Solution}

\subsection{Multivariate Gaussian Integrals}

Ideally, we would like to find a fully analytic solution to this integral. Its form is reminiscent of a gaussian integral in multiple variables. Such integrals are actually quite simple to evaluate analytically. Consider the following:

\begin{equation} \label{eq:multigaus}
\int_{\mathbb{R}^N} \d^N x \hspace{0.5pc} \exp(-\frac{1}{2} \vec{x}^T \Sigma^{-1} \vec{x})
\end{equation}

If we want this integral to converge, $\Sigma^{-1}$ must be positive-definite, which is guaranteed of $\Sigma$ is indeed a covariance matrix. The inner product of $\vec{x}$ through $\Sigma^{-1}$ yields a quadratic polynomial homogeneous in $x_i$, which is exactly what we need for a general multivariate gaussian. Untangling this polynomial appears to be a particularly horrible task, but the trick is to diagonalize.
As $\Sigma$ is a symmetric matrix, so must be its inverse:

\begin{gather*}
(\Sigma \Sigma^{-1})^T = \mathds{I} \\
(\Sigma^{-1})^T \Sigma = \mathds{I} \\
\therefore \hspace{1pc}  (\Sigma^{-1})^T = \Sigma^{-1}
\end{gather*}

the (real) spectral theorem tells us that the diagonal decomposition:

\begin{equation} \label{eq:diagdecomp}
\Sigma^{-1} = R^T D R
\end{equation} 

yields $R$ from $SO(N)$, so we may change basis into the eigenbasis of $\Sigma^{-1}$ with unit jacobian.
Furthermore, a covariance matrix and its inverse are both positive-definite, so $D= D^{1/2} D^{1/2}$.
We may change variables:

\begin{equation} \label{eq:changevar}
\vec{x} \rightarrow \vec{u} = D^{1/2} R \vec{x}
\end{equation}

And now \ref{eq:multigaus} becomes:

\begin{equation} \label{eq:multigaus_trans}
\frac{1}{|\Sigma^{-1}|^{1/2}} \int_{\mathbb{R}^N} \d^N u \hspace{0.5pc} \exp(-\frac{1}{2} \vec{u}^T \vec{u})
\end{equation}

Which can be separated and re-written:

\begin{equation} \label{eq:multigaus_trans}
\frac{1}{|\Sigma^{-1}|^{1/2}} \prod_{i=1}^{N} \int_{\mathbb{R}} \d u_{i} \hspace{0.5pc} \exp(-\frac{1}{2} u_{i}^2)
\end{equation}

Performing the canonical integration:

\begin{equation} \label{eq:multigaus_final}
\frac{1}{|\Sigma^{-1}|^{1/2}} \prod_{i=1}^{N} \int_{\mathbb{R}} \d u_{i} \hspace{0.5pc} \exp(-\frac{1}{2} u_{i}^2) = (2\pi)^{N/2} |\Sigma|^{1/2}
\end{equation}

Polynomial prefactors of the exponent can be obtained by parameter-differentiation, so this technique is quite general. 

The remaining complication is an in-homogeneous polynomial in the exponent. To order 2, this means there may be linear terms. There is a nice trick (variable change) to address this case. What follows is an adaptation of FIXME Cite.

Say we find the following integral:


\begin{equation} \label{eq:multigaus_linterm}
\int_{\mathbb{R}^N} \d^N x \hspace{0.5pc} \exp(-\frac{1}{2} \vec{x}^T \Sigma^{-1} \vec{x} + \vec{J}^T \vec{x})
\end{equation}

Which has the additional linear terms: $\vec{J}^T \vec{X}$. The trick is the change of variables:

$$\vec{x} = \Sigma \vec{J} + \vec{\tilde{x}}$$

The integral becomes, on change-of-variables:

\begin{equation} \label{eq:multigaus_lintrick}
\exp\left(\frac{1}{2}\vec{J}^T \Sigma J\right) \int_{\mathbb{R}^N} \d^N \tilde{x} \hspace{0.5pc} \exp\left(-\frac{1}{2} \vec{\tilde{x}}^T \Sigma^{-1} \vec{\tilde{x}} + \vec{J}^T \vec{\tilde{x}}\right)
\end{equation}

The new integrand has a homogeneous polynomial in its exponent, and will therefore yield to the same separation as \ref{eq:multigaus}.

\subsection{Lambdas First} \label{lambdasec}

With these techniques in hand, let us examine our density integral, ~\ref{eq:allspace_integral} and it's exponenet, ~\ref{eq:Q2}. The first thing to notice is that $Q_2$ is quadratic in $\lambda$, but quartic in $\eta$. Since the gaussian tricks above do not apply to a quartic polynomial, there is no obvious way to proceed in 6 dimensions. This roadblock suggests that we try one of the 3 dimensional subspaces (one accorded to the the three $\lambda$ and the other to the three $\eta$), treating variables from the other as constants. Since the $\lambda$ only appear to order 2, let's start there. Treating the $\eta$ as constants, $Q_2$ is an inhomogeneous polynomial of order 2 in $\eta$, and so should be ammenable to the diagonal separation from above.
\par Indeed, it is possible to diagonalize the matrix which generates this polynomial, and to change variables to the eigenbasis. Unfortunately, we are only interested in field minima, and so our $\lambda$ integrals are not taken over all space. To simplify the limits of integration, we can forget the ordering of $\lambda_i$ and simply integrate each $\lambda$ over $[0,\infty)$. If we were integrating in only one $\lambda$ dimension, we could cast the integral as an evaluation of the gamma function:

\begin{equation} \label{eq:gamma}
\Gamma(t) = \int_{0}^{\infty} x^{t-1} e^{-x} \d x
\end{equation}

Unfortunately, because of the linear terms (a constant vector shift as variable change: \ref{eq:changevar}) and the diagonalization rotation, the limits of integration are no longer simple, even neglecting the $\lambda$ ordering. Call the new variables $u_1$, $u_2$, and $u_3$, in which the polynomial exponent becomes $-\frac{1}{2}(u_1^2 + u_2^2 + u_3^2)$. The limits of integration, even without linear terms, define a paralellpiped in 3-space which is bounded on three faces, and extends infinitely to cover what is a distortion (non-$SO(3)$) linear mapping of the positive octant. The issue is, while $R$ may be an orthogonal operator, $D^{1/2}R$ is not, so the limits of integration of $u_3$ will be a linear function of $u_1$ and $u_2$. $u_2$ will be a linear function of $u_1$, and $u_1$ will be free. Now that the limits of the innermost integral are no longer $0$ and $\infty$, there is no gamma-function trick available.  The innermost integral yields an error function in the next two variables. One integral can be done, as the integrals of $e^{-x^2}$ can be expressed in terms of error functions:

\begin{equation} \label{eq:erfint}
\int \d u_1 erf(u_1) = \frac{e^{-u_1^2}}{\sqrt{\pi}} + u_1 erf(u_1)
\end{equation}

And the diagonalized integrand can be expressed as the product: $\exp(-\frac{1}{2} u_1^2) \exp(-\frac{1}{2} u_2^2) \exp(-\frac{1}{2} u_3^2)$, so the integral can be partially factored:

\begin{equation} \label{eq:gausnest}
A \int_{D1} \d u_1 \exp(-\frac{1}{2} u_1^2) \int_{D2(u_1)} \d u_2 \exp(-\frac{1}{2} u_2^2) \int_{D3(u_1,u_2)} \d u_3 \exp(-\frac{1}{2} u_3^2)
\end{equation}

Where $A$ is a prefactor depending only on $\eta$ and $\nu$, and $D1$,$D2$,$D3$ are domains of integration corresponding to the parallelpiped volume, with $D2(u_1)$ a function of $u_1$, and $D3(u_1,u_2)$ is a funtion of $u_1$ and $u_2$. These domains should be half-infinite, with either upper or lower bounds, depending on the nature of the variable transofrmation. We will call the finite bounds $l_3(u_1,u_2)$, $l_2(u_1)$, and $l_1$ respectively, where $l_i()$ is an affine transformation of its input vector (linear with constant offset) resulting from the origin-shifts and non-orthogonality of the planes partially bounding the integration parallelpiped.

So we can proceed by evaluating the $u_3$ integral in \ref{gausnest}. Unfortunately, evaluating the corresponding antiderivative over the half-infinite interval $D3$ yields a term of the form: $\frac{e^{-l(u_1,u_2)^2}}{\sqrt{\pi}} + l(u_1,u_2) erf(l(u_1,u_2))$. If we then attempt the $u_2$ integral, we need to evaluate the antiderivative: 

\begin{equation*}
\int \d u_2 exp(-\frac{1}{2} u_2^2) \left(frac{e^{-l(u_1,u_2)^2}}{\sqrt{\pi}} + l(u_1,u_2) erf(l(u_1,u_2))\right)
\end{equation*}

And here we spot the cloven hoof. Even the relatively simple function:

\begin{equation} \label{eq:nonointegral}
\exp(-x^2) erf(x+c)
\end{equation}

does not appear to have an anti-derivative. Consulting an impressive compendium of error-function integrals and antiderivatives FIXME CITE OWEN, provides expressions for similar integrals, but they must be taken over particular domains, and I was unable to translate our complicated case (with nested scaling and shift in the erf arguments) into any of these special representations, although that does not mean it is not possible.
It is not obvious to me how to proceed, so let us turn to the $\eta$ integrals and see if any progress can be made there.

\subsection{Eta Integrals}


\begin{gather*}
Q_2 = \half \nu^2 + {3 \over 2 \sigma_1^2} \sum_{i} \nonumber
     \eta_{i}^2
     {5 \sigma_0^2 \over 4 \kappa^2 \sigma_1^4} \left[3
     \left( \sum_i \lambda_i^2 - {2 \over \nu \sigma_0} \sum_i
     \lambda_i  n_i^2 + {1 \over \nu^2 \sigma_0^2} \left(
     \sum_i \eta_i^2 \right)^2\right) - \left(\sum_i
     \lambda_i - {1 \over \nu \sigma_0} \sum_i \eta_i^2
     \right)^2\right]\\ 
     {1 \over 2 \sigma_0^2 (\kappa^2 - 1)}
     \left[\nu \sigma_0 + {\sigma_0^2 \over \sigma_1^2}
     \left(\sum_i \lambda_i - {1 \over \nu \sigma_0} \sum_i
     \eta_i^2 \right)\right]^2 \\ \nonumber
\end{gather*}

The $\eta$ integrals are, by turns, more and less friendly than the $\lambda$ integrals. On one hand, the $Q_2$ polynomial (\ref{eq:Q2}, reprinted above for convenience) is quartic in $\eta_i$, which complicates things. On the other, the domain of integration is all of $\mathbb{R}^3$, which is encouraging, because it suggests that we might be able to avoid the nested-erf roadblock from the lambda integrals. Two angles of attack will be discussed here.

\subsubsection{Change of Variables}

Although $Q_2$ is quartic in $\eta_i$, there are no linear or cubic terms. Furthermore, $\eta_i$ only enters as $\eta_i^2$. Consqeuently, we may rewrite the $\eta$-dependent form of $Q_2$ in inner-product notation in terms not of $\vec{\eta}$, but rather $\vec{\eta^2} = (\eta_1^2,\eta_2^2,eta_3^2)$.

\begin{equation} \label{eq:eta_squared}
Q_2 = -\frac{1}{2} \left(\vec{\eta^2}^T \operatorname{M} \vec{\eta^2} + J^T \vec{\eta^2} + c \right)
\end{equation}

Exploiting the convenient representation of the $\eta_i^2$ in a vector format, we perform a change of variables.

$$v_i = \eta_i^2$$

This is a transformation, component-wise, of the $\vec{\eta^2}$ vector into a vector $\vec{v}$.

This transformation yields the jacobian matrix:

\begin{equation}
\frac{\partial v_i}{\partial \eta_j} = 2*(v_i)^{1/2} \delta^{i}_{j}
\end{equation}

Incorporating the corresponding jacobian factor, the eta integrals become, upon change-of-variables:

\begin{equation} \label{eq:eta_lintrick}
\exp\left(\frac{1}{2}\vec{J}^T \Sigma J\right) \int_0^{\infty} \d v_1 \int_0^{\infty} \d v_2 \int_0^{\infty} \d v_3 \hspace{0.5pc}  (64 v_1 v_2 v_3)^{-1/2} \exp\left(-\frac{1}{2} \vec{v}^T \Sigma^{-1} \vec{v} + \vec{J}^T \vec{v}\right)
\end{equation}

To remove the term linear in $v_i$, we use the trick we applied to \ref{eq:multigaus_linterm}:

$$\vec{v} = \operatorname{M}^{-1} \vec{J} + \vec{\tilde{v}}$$

Which substitutiion yields:

\begin{equation} 
\exp\left(\frac{1}{2}\vec{J}^T \operatorname{M}^{-1} J\right) \int_{-w_1}^{\infty} \d \tilde{v_1} \int_{-w_2}^{\infty} \d \tilde{v_2} \int_{-w_3}^{\infty} \d \tilde{v_3}  (64 (\tilde{v}_1 + w_1)(\tilde{v}_2 + w_2)(\tilde{v}_2 + w_2))^{-1/2}  \exp\left(-\frac{1}{2} \vec{\tilde{v}}^T \operatorname{M} \vec{\tilde{v}} + \vec{J}^T \vec{\tilde{v}}\right)
\end{equation}

Where $w_i = {(\operatorname{M}^{-1})_i}^j J_j$

In preparing our integral for diagonalization, we have fallen into the same nested-erf trap as in Section \ref{lambdasec}! The change of variables to $v_i = \eta_i^2$ changed the limits of integration to half-lines. As a result, diagonalization will scramble the limits, yielding the same kind of parallelpiped trouble as before.

As a last-ditch attempt to salvage this integration technique, one might consider the $64 (\tilde{v}_1 + w_1)(\tilde{v}_2 + w_2)(\tilde{v}_2 + w_2))^{-1/2}$ factor as a handle for contour integration in the complex plane. Unfortunately, although this jacobian factor does have singularities in each $\tilde{v}_i$ plane, and the complementary factor is an entire function, these singularities are not poles, but rather boundaries of branch cuts. Without any poles, it does not appear to be possible to do contour integration.

\subsubsection{Multilinear Forms}

In our efforts to write order 2 polynomials as inner products of vectors across some $\Sigma^{-1}$ matrix, we have been making implicit use of a bijective relationship between quadratic homogeneous (all terms have the same total order) polynomials in $n$ variables and inner products of the form: $\vec{x}^T \operatorname{\Sigma}^{-1} \vec{x}$ where $\operatorname{\Sigma}^{-1}$ is a symmetric matrix. Such matrices, in this context, are representations of a map of a tensor product of vector spaces onto their underlying field: $F: V \otimes V \rightarrow \mathbb{R}$ (the reals, in our case). The motivation for this point of view is that it can be generalized to multi-linear forms, which are capable of mapping tensor products of more than two vector spaces onto homogeneous polynomials of arbitrary degree. These multi-linear forms have representations as tensors, and in the case of symmetric tensors, there is, once again, a bijection between homogeneous polynomials of degree $k$ and symmetric tensors of order $k$. The connection is made clearer when these tensors are used as multilinear forms to map elements of the vector space: $V^{\otimes k}$ onto $\mathbb{R}$. To visualize the generalization, let's begin with the order-2 (matrix case).

Let $\mathcal{P}_2(x)$ be a homogeneous polynomial of order 2, where $x\in V_2(\mathbb{R})$, which is to say x is a vector in a real vector space of dimension $2$.

Generally:

$$ \mathcal{P}_2(x) =  \alpha x_1^2 + \beta x_1 x_2 + \gamma x_2^2 $$

The standard (euclidean) inner product, which is a bilinear form represented here as the identity on $V_2(\mathbb{R})$, will produce the following polynomial:

$$ \sum_{i,j} \mathds{1}_{ij} x_i x_j = x_1^2 + x_2^2$$

So, if we want to be able to generate cross-terms, and to apply to each term an arbitrary prefactor (to produce more general polynomials), we need to replace $\mathds{1}$ with a more general matrix. In this case, to generate any $\mathcal{P}_2(x)$, we can use the matrix:


\[M = \left(\begin{array}{ccc}
\alpha & \beta/2 \\
\beta/2 & \gamma \\
\end{array} \right)\] 

Employing $M$ as a bilinear form, we find:

\begin{equation} \label{eq:bilinear_form}
\mathcal{P}_2(x) =  \sum_{i,j} M_{ij} x_i x_j = \alpha x_1^2 + \beta x_1 x_2 + \gamma x_2^2 
\end{equation}


What if we want to work with higher-degree polynomials (as in the quartic $\eta$ case)?
The above correspondence suggests a relationship between symmetric multilinear forms (represented as order-$k$ tensors) and homogeneous polynomials of degree $k$. Indeed, there is a bijection between these two classes of mathematical objects (FIXME CITE SYMMETRIC):

\begin{equation} \label{eq:multilinear_form}
\mathcal{P}_k(x) =  \sum_{i_1...i_k} M_{i_1...i_k} x^{i_1}...x^{i_k}
\end{equation}

Where the requirement that $M$ is a symmetric tensor translates into the statement that M, represented in some set of indices, is invariant under a permutation of a particular index-vector:

$$ M_{i_1...i_k} = M_{\sigma(i_1,...,i_k)}$$

Where $\sigma()$ is any element of the permutation group of $k$ elements.

Which is an elegant formulation, but what is the application to our $\eta$ integration?

We were able to decouple the integrals in the gaussian case by diagonalizing the matrix representation of a symmetric bilinear form, $\Sigma^{-1}$ (\ref{eq:multigaus}) and changing variables accordingly, might it be possible to ``diagonalize'' the tensor representation of the quartic form in $\eta$ and change variables in the $\eta$ vector space to decouple the integral into a product of integrals, each of which could perhaps be turned into an evaluation of the gamma function?

Let us, for the time being, neglect the inhomogeneous terms in the $Q_2$ polynomial (it may be possible to perform a trick like that which was done in the gaussian case to kill the linear terms), and consider the homogeneous case.

As it happens, symmetric tensors do admit a ``diagonal'' decomposition, sometimes called the \textit{Waring decomposition}. Let $T$ be a tensor of order $k$, then there exist $v_i\in V_k(\mathbb{R})$ such that:

\begin{equation} \label{eq:waring}
T = \sum_{i=1}^r \lambda_i v_i^{\otimes k}
\end{equation} 

This is, essentially, an eigenvalue/eigen-(rank-1)tensor decomposition.

Recall that our homogeneous polynomial, $\mathcal{P}_4(\eta)$ has representation:

\begin{equation} \label{eq:etaform}
\mathcal{P}_4(\eta) =  \sum_{i_1...i_k} T_{i_1...i_k} \eta^{i_1}...\eta^{i_k}
\end{equation}

$T$ is symmetric and order-$4$, so this can be written:

\begin{equation} \label{eq:etaform_eigen}
\mathcal{P}_4(\eta) =  \sum_{i_1...i_k} \left(\sum_{\alpha=1}^r \lambda_{\alpha} v_{\alpha}^{\otimes k}\right)_{i_1...i_k} \eta^{i_1}...\eta^{i_k}
\end{equation}

As a reminder, $i_l$ is the $l$th index of both the $\eta$ tensor we're contracting with $T$, and the $v_i^{\otimes k}$ eigentensor of $T$. But, because both the $\eta$ and $v$ tensors are k-th tensor products of a single vector, $\eta$ and $v_i$ respectively, they have simple forms in index notation:

\begin{gather*}
\left(\eta^{\otimes k}\right)^{i_1...i_k} = \eta^{i_1}...\eta^{i_k} \\ 
\left(v_{\alpha}^{\otimes k}\right)^{i_1...i_k} = v_{\alpha}^{i_1}...v_{\alpha}^{i_k} \\
\end{gather*}

So we may rewrite \ref{eq:etaform_eigen}, exchanging summations:

\begin{equation} \label{eq:etaform_eigen}
\mathcal{P}_4(\eta) =  \sum_{\alpha=1}^r \lambda_{\alpha} \sum_{i_1...i_k} \left(v_{\alpha}\right)_{i_1}...\left(v_{\alpha}\right)_{i_k} \eta^{i_1}...\eta^{i_k}
\end{equation}

Notice that the innermost summand is simply a power of inner products!
Exploiting a combination of standard and Einstein summation notations:


\begin{equation} \label{eq:etaform_eigen_niii}
\mathcal{P}_4(\eta) =  \sum_{\alpha=1}^r \lambda_{\alpha} \left(\left(v_{\alpha}\right)_i\eta^i\right)^4
\end{equation}

And this can be written in the form:

\begin{equation} \label{eq:etaform_eigen_niii}
\mathcal{P}_4(\eta) =  \delta_{\alpha \beta \gamma \delta} \left(\left(\Lambda^{1/4}\right)^{\alpha}_{j} W^{j}_i\eta^i\right)\left(\left(\Lambda^{1/4}\right)^{\beta}_{j} W^{j}_i\eta^i\right)\left(\left(\Lambda^{1/4}\right)^{\gamma}_{j} W^{j}_i\eta^i\right)\left(\left(\Lambda^{1/4}\right)^{\delta}_{j} W^{j}_i\eta^i\right)
\end{equation}

Where:

\[W = \left(\begin{array}{c}
\vec{v_1} \\
\vec{v_2} \\
\vec{v_3} \\
\vec{v_4} \\
\end{array} \right)\] 

And

\[\Lambda = \left(\begin{array}{cccc}
\lambda_1 & 0 & 0 & 0 \\
0 & \lambda_2 & 0 & 0 \\
0 & 0 &\lambda_3 & 0 \\
0 & 0 & 0 & \lambda_4 \\
\end{array} \right)\] 


Finally, we have arrived at our diagonal destination. With the variable change:

$$ \vec{u} = \Lambda^{1/4} W \vec{\eta} $$

The polynomial $\mathcal{P}_4(\eta)$ becomes:

\begin{equation} \label{eq:etaform_eigen_niii}
\mathcal{P}_4(\eta) =  \delta_{\alpha \beta \gamma \delta} u^{\alpha} u^{\beta} u^{\gamma} u^{\delta} = u_1^4 + u_2^4 + u_3^4 + u_4^4
\end{equation}

This appears to be cause for celebration, as the integral would now be seperable, and (crucially) the limits are still full real lines for each $\eta$, so the variable change will not introduce any of this parallelogram nonsense. However, there are two snags which I have not overcome.

The first is the question of how to remove the quadratic terms to homogenize this polynomial. I have some ideas, but have not yet been able to clarify the procedure (it might not be do-able: this needs investigation).

The second snag is more fundamental, and, if solvable, will require serious mathematical gymnastics.

Earlier, when introducing the Waring tensor decomposition \ref{eq:waring}, I slipped something under the radar. In the order-2 case, the waring decomposition for a real symmetric tensor is the diagonal decomposition for a real symmetric matrix. We are guaranteed that the number of eigen-tensors in the decomposition sum (tensor products of the eigen-vectors in the matrix case, call this number $r$) is exactly equal to the dimension of the underlying vector space ($k$).

This decomposition, in tensor notation, looks like this:

\begin{equation} \label{eq:waring-o2}
T = \sum_{i=1}^d \lambda_i v_i^{\otimes 2}
\end{equation} 

Where $v_i \in V_d(\mathbb{R})$.

But the crucial difference between this special case and the Waring decomposition is that $r=d$. There are never more terms in the decomposition than there are dimensions in the underlying vector space $V_d(\mathbb{R})$. However, in higher-order decompositions, the rule is: $r\geq d$. And if there are more terms in the decomposition than there are dimensions in the vector space: A) the $v_i$ cannot be linearly independent, and B) we cannot write down $W$ as a 3-by-3 matrix. It would appear that we need another $\eta$ dimension to effect the change of variables. I don't know if it's possible to embed our $\eta$ integral in a higher dimensional integral and then extract the 3-d integral after the fact.

Unfortunately, I think this isn't possible (though this needs to be checked), because building the $W$ matrix out of linearly dependent rows will cause it to have a rank less than $r$, which, in an $r$ by $r$ matrix, implies determinant zero, and the change of variables is not invertible, and therefore invalid. Although, it is possible that in some higher dimensional space, $r=d$, and the construction is valid.

 Furthermore, in the order-2 case, the real spectral theorem guarantees that these vectors are mutually orthogonal, so that the $W$ matrix is an orthogonal operator. It is not obvious to me that, even if $r=d$, there is still a spectral theorem for these decompositions. The jacobian of the transformation will therefore introduce a nasty function of the $u_i$ in front of the exponential. 

 Moving forward along this line of thinking will probably require a thorough literature search, beginning with FIXME REF SYMMETRIC.











%\ref{eq:etaform_eigen} has an evil look about it, so let's consider the order-2 (matrix) case:

%\begin{equation} \label{eq:etaform_o2}
%\mathcal{P}_4(\eta) =  \sum_{\alpha=1}^r \lambda_{\alpha} \sum_{i_1,i_2} \left(v_{\alpha}\right)_{i_1}%%%\left(v_{\alpha}\right)_{i_2} \eta^{i_1}eta^{i_2}
%\end{equation}

%Writing this in Dirac notation, we see a familiar face of the eigen-decomposition!


%\begin{equation} \label{eq:etaform_dirac}
%\sum_{\alpha=1}^r \lambda_{\alpha} \sum_{i_1,i_2} \left(v_{\alpha}\right)_{i_1}\left(v_{\alpha}\right)_{i_2} %\eta^{i_1}\eta^{i_2} = \bra{\eta} \left(\sum_{\alpha=1}^r \ket{v_{\alpha}} \lambda_{\alpha} \bra{v_\alpha} %\right) \ket{\eta} = \bra{\eta} T \ket{\eta}
%\end{equation}

%The analogy to the order-2 case guides our next step. 

\section{A Numerical Solution}

\subsection{•}






%BIB


%@article{doi:10.1137/060661569,
%author = {Pierre Comon and Gene Golub and Lek-Heng Lim and Bernard Mourrain},
%title = {Symmetric Tensors and Symmetric Tensor Rank},
%journal = {SIAM Journal on Matrix Analysis and Applications},
%volume = {30},
%number = {3},
%pages = {1254-1279},
%year = {2008},
%doi = {10.1137/060661569},

%URL = { 
%        http://dx.doi.org/10.1137/060661569
%    
%},
%eprint = { 
%%        http://dx.doi.org/10.1137/060661569
%   
%}%
%
%}




%@article{doi:10.1080/03610918008812164,
%author = {   D. B.   Owen },
%title = {A table of normal integrals},
%journal = {Communications in Statistics - Simulation and Computation},
%volume = {9},
%number = {4},
%pages = {389-419},
%year = {1980},
%doi = {10.1080/03610918008812164},

%URL = { 
%        http://dx.doi.org/10.1080/03610918008812164
%    
%},
%eprint = { 
%        http://dx.doi.org/10.1080/03610918008812164
%    
%}
%
%}




\end{document}